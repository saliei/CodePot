{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This here is to keep main ideas while I go through [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com) book, real quick:\n",
    "\n",
    "* **perceptrons**: \n",
    "\n",
    "    * a **step function**\n",
    "    * by varying the **bias**(-threshold) we can achieve different set of decision making, regardless of weights.\n",
    "    * high(positive) bias means that it's very easy to make perceptron to fire, and vice versa:\n",
    "    $\\begin{eqnarray}\n",
    "  \\mbox{output} = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } w\\cdot x + b \\leq 0 \\\\\n",
    "      1 & \\mbox{if } w\\cdot x + b > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "    * you can use perceptrons to simulate a NAND gate, they are **unviersal for computation**, hence perceptrons are also universal for computation. you can use them to simulate any function! they are as good as any computing device. this is hardly a big news!\n",
    "    * BUT you can device *learning algorithms* that would lay out this \"circuts of gates\"(neural net with appropriate bias and weights) for you. this makes the difference.\n",
    "    * what would make learning possible(weights and bias) for the network is the property that small change in the weights and bias makes a small change in the output of the net.\n",
    "    * for a step function this is not possible, you need a smooth function **activation function**, hence **sigmoid** function.\n",
    "* **sigmoid neurons**:\n",
    "   \n",
    "    * sigmoid function $\\sigma(z)=\\frac{1}{1+\\exp(-z)}$\n",
    "    * can take real values instead of 1 and 0 for input and output\n",
    "    \n",
    "* **feed forward networks**: information is always fed forward, there is no loop. otherwise output will depend on the input. if there is loop it is called **recurrent neural network**, but looping is not instant, the ouput will be fed to input at some later time. \n",
    "\n",
    "* to determine the input layer dimension we look at the data dimesnion, e.g. for a $64\\times64$ grey scale image, the input layer is 4096 dimension with each value being the intensity of the pixel.\n",
    "\n",
    "* **segmentation problem** break up a sequence of numbers to stand alone numbers(images)\n",
    "* **cost function(loss function, objective function)**: an example: \n",
    "$\\begin{eqnarray}  C(w,b) \\equiv\n",
    "  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2.\n",
    "\\end{eqnarray}$ *quadratic cost function* or *mean squared error*\n",
    "* **gradient descent** an algorithm to minimize cost function. you always want to have cost fucntion decreasing, so, $C:=C(\\vec\\nu), \\Rightarrow \\Delta C = \\nabla C\\cdot\\Delta\\nu$ so if we choose, $\\Delta\\nu = -\\eta\\nabla C$, $\\eta$ is **learning rate**. we will use this equation to update new positons(values), $\\nu^\\prime = \\nu - \\eta\\nabla C$, if learning rate is too big we have a problem with approximation, we may end up increasing cost function, if it's too small, it will take too much time to find the minimum. gradient descent update rule:\n",
    "$ w_k \\rightarrow w_k' = w_k-\\eta \\frac{\\partial C}{\\partial w_k} $ and  $ b_l \\rightarrow b_l' = b_l-\\eta \\frac{\\partial C}{\\partial b_l} $\n",
    "\n",
    "* cost function is an average over cost function for individual cost function over each training example. computing gradient over large training dataset would take much more time. hence **stochastic gradient descent**, the idea is to sample a random mini-batch over training example and use: \n",
    "$ \\begin{eqnarray}\n",
    "  \\frac{\\sum_{j=1}^m \\nabla C_{X_{j}}}{m} \\approx \\frac{\\sum_x \\nabla C_x}{n} = \\nabla C,\n",
    "\\end{eqnarray}$\n",
    "\n",
    "* stochastic gradient descent works by picking out a randomly chosen mini-batch of training inputs, and training with those. Then we pick out another randomly chosen mini-batch and train with those. And so on, until we've exhausted the training inputs, which is said to complete an epoch of training. At that point we start over with a new training epoch. if the batch size is one(like humans), then it is called *online* or *on-line* or incremental learning.\n",
    "\n",
    "* people use *validation* data to set *hyper-parameters* of the model, learning rate, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Main equations of backpropagation:\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  a^{l}_j = \\sigma\\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right),\n",
    "\\tag{1}\\end{eqnarray}$\n",
    "\n",
    "activation\n",
    "$\\begin{eqnarray} \n",
    "  a^{l} = \\sigma(w^l a^{l-1}+b^l).\n",
    "\\tag{2}\\end{eqnarray}$\n",
    "\n",
    "weighted ouput\n",
    "$\\begin{eqnarray}\n",
    "z^l_j = \\sum_k w^l_{jk} a^{l-1}_k+b^l_j\n",
    "\\tag{3}\\end{eqnarray}$\n",
    "\n",
    "the error in node j of layer l\n",
    "$\\begin{eqnarray} \n",
    "  \\delta^l_j \\equiv \\frac{\\partial C}{\\partial z^l_j}.\n",
    "\\tag{4}\\end{eqnarray}$\n",
    "\n",
    "error in the output layer\n",
    "$\\begin{eqnarray} \n",
    "  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j).\n",
    "\\tag{BP1}\\end{eqnarray}$\n",
    "\n",
    "error in layer $l$ based on error in layer $l+1$\n",
    "$\\begin{eqnarray} \n",
    "  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l),\n",
    "\\tag{BP2}\\end{eqnarray}$\n",
    "By combining (BP2) with (BP1) we can compute the error δl for any layer in the network. We start by using (BP1) to compute δL, then apply Equation (BP2) to compute δL−1, then Equation (BP2) again to compute δL−2, and so on, all the way **back** through the network.\n",
    "\n",
    "$\\begin{eqnarray}  \\frac{\\partial C}{\\partial b^l_j} =\n",
    "  \\delta^l_j.\n",
    "\\tag{BP3}\\end{eqnarray}$\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j.\n",
    "\\tag{BP4}\\end{eqnarray}$\n",
    "\n",
    "* see the backpropagation algorithm, pseudocode: [link](http://neuralnetworksanddeeplearning.com/chap2.html#the_backpropagation_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"sizes is a list of layers, each element corresponds to the size of the layer.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(x, 1) for x in sizes[1:]]\n",
    "        self.weights = [np.random.randn(x, y) for x, y in zip(sizes[1:], sizes[:-1])]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "            print(a)\n",
    "        return a\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "        \n",
    "    def SGD(self, training_data, batch_size, epochs, eta, test_data=None):\n",
    "        \"\"\"stochastic gradient descent algorithm. randomly select a mini batch from\n",
    "        training data, learn from it, do this again until all training data is exausted,\n",
    "        repeat this for epoch times. the training data is a list of tuples `(x, y)`.\n",
    "        y being the labels\"\"\"\n",
    "        test_data = list(test_data)\n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        for i in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            batches = [training_data[k:k+batch_size] for k in np.random.randn(0, n, batch_size)]\n",
    "            for batch in batches:\n",
    "                self.update_batch(batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch: {}, {}/{} \".format(i, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch: {} completed.\".format(i))\n",
    "                \n",
    "        def update_batch(self, batch, eta):\n",
    "            nabla_b = [np.zeros(np.shape(b)) for b in self.biases]\n",
    "            nabla_w = [np.zeros(np.shape(w)) for w in self.weights]\n",
    "            for x, y in batch:\n",
    "                delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "                nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "                nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            self.biases = [b-(eta/batch_size)*nb for b, nb in zip(self.biases, delta_nabla_b)]\n",
    "            self.weights = [w-(eta/batch_size)*nw for w,nw in zip(self.weights, delta_nabla_w)]\n",
    "        \n",
    "        def backprop(self, x, y):\n",
    "            nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "            nabla_w = [np.zeros(w.shape) for w in self.biases]\n",
    "            activation = x\n",
    "            activations = [x] \n",
    "            zs = [] \n",
    "            #forward pass\n",
    "            for b, w in zip(self.biases, self.weights):\n",
    "                z = np.dot(w, activation)+b\n",
    "                zs.append(z)\n",
    "                activation = sigmoid(z)\n",
    "                activations.append(activation)\n",
    "            #backward pass\n",
    "            delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "            nabla_b[-1] = delta\n",
    "            nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "            \n",
    "            for l in xrange(2, self.num_layers):\n",
    "                z = zs[-l]\n",
    "                sp = sigmoid_prime(z)\n",
    "                delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "                nabla_b[-l] = delta\n",
    "                nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            \n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "        def cost_derivative(self, output_activations, y):\n",
    "             return (output_activations-y)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def fetch(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        dat = f.read()\n",
    "    return np.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy()\n",
    "X_train = fetch(\"data/mnist/train-images-idx3-ubyte.gz\")[0x10:]#.reshape((-1, 28, 28))\n",
    "Y_train = fetch(\"data/mnist/train-labels-idx1-ubyte.gz\")[8:]\n",
    "X_test = fetch(\"data/mnist/t10k-images-idx3-ubyte.gz\")[0x10:]#.reshape((-1, 28, 28))\n",
    "Y_test = fetch(\"data/mnist/t10k-labels-idx1-ubyte.gz\")[8:]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.imshow(X_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data = [(x, y) for x, y in zip(X_train, Y_train)]\n",
    "#test_data = [(x, y) for x, y in zip(X_test, Y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_data():\n",
    "    f = gzip.open('./data/mnist/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 completed.\n",
      "Epoch: 1 completed.\n",
      "Epoch: 2 completed.\n",
      "Epoch: 3 completed.\n",
      "Epoch: 4 completed.\n",
      "Epoch: 5 completed.\n",
      "Epoch: 6 completed.\n",
      "Epoch: 7 completed.\n",
      "Epoch: 8 completed.\n",
      "Epoch: 9 completed.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nabla_b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-fc9256c68e45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-76-93ffa2c7f7e1>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, batch_size, epochs, eta, test_data)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mnabla_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnabla_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcost_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nabla_b' is not defined"
     ]
    }
   ],
   "source": [
    "nn = Network([784, 30, 10])\n",
    "nn.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* cross entropy cost function:\n",
    "$\\begin{eqnarray} \n",
    "  C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right],\n",
    "\\end{eqnarray}$\n",
    "* softmax layer\n",
    "$\\begin{eqnarray} \n",
    "  a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}},\n",
    "\\end{eqnarray}$ $z^L_j = \\sum_{k} w^L_{jk} a^{L-1}_k + b^L_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
